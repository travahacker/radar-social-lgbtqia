import gradio as gr
import pandas as pd
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pickle
import json
import re
import hashlib
import os
import warnings

warnings.filterwarnings("ignore")

# --- ConfiguraÃ§Ãµes ---
DEVICE = "cpu"  # Simplificado para evitar problemas de GPU
MODEL_PATH = "Veronyka/radar-social-lgbtqia"

# --- NormalizaÃ§Ã£o de Texto ---
def normalize_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+|https\S+", "[URL]", text, flags=re.MULTILINE)
    text = re.sub(r"@\w+", "[MENTION]", text)
    text = re.sub(r"#\w+", "[HASHTAG]", text)
    text = re.sub(r"[^\w\s\[\]]", "", text) # Remove pontuaÃ§Ã£o, mas mantÃ©m []
    text = re.sub(r"\s+", " ", text).strip()
    return text

# --- REGRAS ESPECÃFICAS PARA CASOS PROBLEMÃTICOS ---
def detect_neutral_language_opposition(text):
    """Detecta oposiÃ§Ã£o Ã  linguagem neutra"""
    text_lower = text.lower()
    
    patterns = [
        r'\btodes\b.*\b(Ã©|sÃ£o|foi|era)\b.*\b(meu|meus|minha|minhas)\b.*\b(ovo|ovos|egg|eggs)\b',
        r'\b(quem|pessoa).*\bfala\b.*\btodes\b.*\b(retardado|retardades|burro|burra)\b',
        r'\btodes\b.*\b(fim da picada|babaquice|idiota|burro)\b',
        r'\b(modinha|frescura)\b.*\b(todes|linguagem neutra)\b'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_clown_emoji_context(text):
    """Detecta contexto de emojis de palhaÃ§o"""
    patterns = [
        r'ğŸ˜‚+.*\b(todes|linguagem neutra|neutral)\b',
        r'\b(todes|oves|lules)\b.*ğŸ˜‚+',
        r'ğŸ˜‚+.*\b(ovo|ovos|egg|eggs)\b',
        r'\b(ovo|ovos|egg|eggs)\b.*ğŸ˜‚+'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text):
            return True
    return False

def detect_curse_words_neutral_context(text):
    """Detecta palavrÃµes em contexto neutro"""
    text_lower = text.lower()
    
    patterns = [
        r'\b(porra|merda|bosta)\b.*\b(todes|linguagem neutra)\b',
        r'\b(meu amigo|mano|gente)\b.*\b(porra|merda|bosta)\b',
        r'\b(porra|merda|bosta)\b.*\b(que|isso|essa)\b.*\b(coisa|situaÃ§Ã£o)\b',
        r'\b(porra|merda|bosta)\b.*\btodes\b.*\.\.\.',  # "Porra de todes..."
        r'\b(porra|merda|bosta)\b.*\btodes\b$'  # "Porra de todes" (final da frase)
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_tiredness_expressions(text):
    """Detecta expressÃµes de cansaÃ§o/desÃ¢nimo"""
    text_lower = text.lower()
    
    patterns = [
        r'\b(que|que) (preguiÃ§a|cansaÃ§o|desÃ¢nimo|fadiga)\b',
        r'\b(preguiÃ§a|cansaÃ§o|desÃ¢nimo|fadiga)\b.*\b(desse|dessa|disso)\b',
        r'\b(estou|tÃ´|estou) (cansado|cansada|exausto|exausta)\b'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_religious_neutral_expressions(text):
    """Detecta expressÃµes religiosas neutras"""
    text_lower = text.lower()
    
    patterns = [
        r'\b(meu|ai) deus\b',
        r'\bnossa senhora\b',
        r'\bdeus do cÃ©u\b',
        r'\b(ai|meu) deus\b.*\b(que|isso|essa)\b'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_vomit_emoji_context(text):
    """Detecta contexto de emojis de vÃ´mito"""
    patterns = [
        r'ğŸ¤¢ğŸ¤®',
        r'ğŸ¤®ğŸ¤¢',
        r'ğŸ¤¢.*ğŸ¤®',
        r'ğŸ¤®.*ğŸ¤¢'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text):
            return True
    return False

def detect_laughter_context_neutral(text):
    """Detecta emojis de risada em contexto neutro"""
    patterns = [
        r'ğŸ˜‚+.*\b(insignificante|sacanagem|brincadeira|piada)\b',
        r'\b(insignificante|sacanagem|brincadeira|piada)\b.*ğŸ˜‚+',
        r'ğŸ˜‚+$',  # Apenas emojis de risada no final
        r'^ğŸ˜‚+$'  # Apenas emojis de risada
    ]
    
    for pattern in patterns:
        if re.search(pattern, text):
            return True
    return False

def detect_generation_expressions(text):
    """Detecta expressÃµes sobre geraÃ§Ãµes/idades"""
    text_lower = text.lower()
    
    patterns = [
        r'\b(geraÃ§Ã£o|geracao)\b.*\b(fraca|fracassada|fracassado|nova|velha)\b',
        r'\b(fraca|fracassada|fracassado|nova|velha)\b.*\b(geraÃ§Ã£o|geracao)\b',
        r'\b(geraÃ§Ã£o|geracao)\b.*\b(perdida|sem futuro|sem rumo)\b'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_male_genital_machismo(text):
    """Detecta machismo atravÃ©s de termos de genitais masculinos"""
    text_lower = text.lower()
    
    patterns = [
        r'\b(ovo|ovos|egg|eggs)\b.*\b(meu|minha|meus|minhas)\b',
        r'\b(roles|rola|pinto)\b.*\b(meu|minha|meus|minhas)\b',
        r'\b(meu|minha|meus|minhas)\b.*\b(ovo|ovos|egg|eggs|roles|rola|pinto)\b'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_clown_emoji_isolated(text):
    """Detecta emoji de palhaÃ§o isolado"""
    patterns = [
        r'^ğŸ¤¡$',
        r'ğŸ¤¡$',
        r'^ğŸ¤¡'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text):
            return True
    return False

def detect_care_expressions(text):
    """Detecta expressÃµes de cuidado/consolo"""
    text_lower = text.lower()
    
    patterns = [
        r'\b(vai|vai)\b.*\b(tomar|tomar)\b.*\b(remÃ©dio|remedio)\b',
        r'\b(vai|vai)\b.*\b(dormir|descansar)\b',
        r'\b(cuide|cuida)\b.*\b(si|de si)\b',
        r'\b(descanse|descansa)\b'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_neutral_curse_words(text):
    """Detecta palavrÃµes em contexto neutro"""
    text_lower = text.lower()
    
    patterns = [
        r'\b(fala|falar)\b.*\b(bosta|merda|porra)\b',
        r'\b(bosta|merda|porra)\b.*\b(fala|falar)\b',
        r'^bosta$',
        r'^merda$',
        r'^porra$'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_disapproval_without_hate(text):
    """Detecta expressÃµes de desaprovaÃ§Ã£o sem Ã³dio"""
    text_lower = text.lower()
    
    patterns = [
        r'\b(palhaÃ§ada|palhacada)\b',
        r'\b(patÃ©tico|patetico)\b',
        r'\b(hilarious)\b',
        r'\b(ridÃ­culo|ridiculo)\b'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_generic_insults_without_context(text):
    """Detecta insultos genÃ©ricos sem contexto especÃ­fico"""
    text_lower = text.lower()
    
    patterns = [
        r'\b(analfabetos|analfabeto)\b.*\b(funcionais|funcional)\b',
        r'\b(retardades|retardado)\b',
        r'\b(vermes|verme)\b',
        r'\b(imbecis|imbecil)\b'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_neutral_emoji_context(text):
    """Detecta emojis em contexto neutro"""
    patterns = [
        r'ğŸ˜‚+.*\b(hilarious|engraÃ§ado|divertido)\b',
        r'\b(hilarious|engraÃ§ado|divertido)\b.*ğŸ˜‚+'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text):
            return True
    return False

def detect_neutral_language_specific_cases(text):
    """Detecta casos especÃ­ficos de linguagem neutra"""
    text_lower = text.lower()
    
    patterns = [
        r'\btodes\b.*\b(meus|minhas)\b.*\b(ovos|ovo)\b.*\.\.',
        r'\b(meus|minhas)\b.*\b(ovos|ovo)\b.*\btodes\b.*\.\.'
    ]
    
    for pattern in patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_enhanced_male_genital_machismo(text):
    """Detecta machismo atravÃ©s de genitais masculinos com alta prioridade"""
    text_lower = text.lower()
    
    # PadrÃµes com possessivos masculinos
    possessive_patterns = [
        r'\b(meu|meus)\b.*\b(ovo|ovos|oves|egg|eggs)\b',
        r'\b(ovo|ovos|oves|egg|eggs)\b.*\b(meu|meus)\b',
        r'\b(meu|meus)\b.*\b(roles|rola|pinto|pintos)\b',
        r'\b(roles|rola|pinto|pintos)\b.*\b(meu|meus)\b'
    ]
    
    # VariaÃ§Ãµes ortogrÃ¡ficas (apenas quando em contexto de posse)
    spelling_variations = [
        r'\b(meuzovos|meusoves|meuzoves)\b',
        r'\b(oves|eggs)\b.*\b(meu|meus)\b',
        r'\b(meu|meus)\b.*\b(oves|eggs)\b',
        r'\b(roles|rola|pinto)\b.*\b(meu|meus)\b',
        r'\b(meu|meus)\b.*\b(roles|rola|pinto)\b'
    ]
    
    # PadrÃµes em contexto de linguagem neutra
    neutral_language_context = [
        r'\btodes\b.*\b(meu|meus)\b.*\b(ovo|ovos|oves|egg|eggs)\b',
        r'\b(meu|meus)\b.*\b(ovo|ovos|oves|egg|eggs)\b.*\btodes\b',
        r'\btodes\b.*\b(roles|rola|pinto|pintos)\b',
        r'\b(roles|rola|pinto|pintos)\b.*\btodes\b'
    ]
    
    all_patterns = possessive_patterns + spelling_variations + neutral_language_context
    
    for pattern in all_patterns:
        if re.search(pattern, text_lower):
            return True
    return False

def detect_enhanced_neutral_language_hate(text):
    """Detecta Ã³dio contra linguagem neutra - VERSÃƒO MELHORADA"""
    text_lower = text.lower()
    
    # PadrÃµes de Ã³dio especÃ­fico Ã  linguagem neutra (apenas quando em contexto de Ã³dio)
    hate_neutral_language_patterns = [
        r'\b(que|que)\b.*\b(porcarie|porcarias)\b',
        r'\b(porcarie|porcarias)\b.*\b(que|que)\b',
        r'\b(todes|lules|mussum)\b.*\b(que|que)\b.*\b(porcarie|porcarias|nojento|escroto|desgraÃ§ado)\b',
        r'\b(que|que)\b.*\b(porcarie|porcarias)\b.*\b(todes|lules|mussum)\b',
        r'\b(modinha|frescura|babaquice)\b.*\b(todes|lules|linguagem neutra)\b',
        r'\b(todes|lules|linguagem neutra)\b.*\b(modinha|frescura|babaquice|idiota|burro)\b',
        r'\b(fim da picada|chega|basta)\b.*\b(todes|lules|linguagem neutra)\b',
        r'\b(todes|lules|linguagem neutra)\b.*\b(fim da picada|chega|basta|para)\b'
    ]
    
    all_patterns = hate_neutral_language_patterns
    
    for pattern in all_patterns:
        if re.search(pattern, text_lower):
            return True
    return False

# --- REGRAS CONTEXTUAIS PARA TERMOS LGBTQIA+ ---
def detect_positive_context(text):
    """Detecta contexto positivo para termos LGBTQIA+"""
    text_lower = text.lower()
    
    # Palavras positivas que indicam contexto de respeito/aceitaÃ§Ã£o
    positive_indicators = [
        'orgulho', 'pride', 'amor', 'love', 'respeito', 'respect',
        'beleza', 'beautiful', 'lindo', 'beautiful', 'maravilhoso', 'wonderful',
        'coragem', 'courage', 'forÃ§a', 'strength', 'identidade', 'identity',
        'expressÃ£o', 'expression', 'liberdade', 'freedom', 'direito', 'right',
        'aceitar', 'accept', 'aceitar', 'embrace', 'celebrar', 'celebrate',
        'apoio', 'support', 'solidariedade', 'solidarity', 'comunidade', 'community',
        'visibilidade', 'visibility', 'representaÃ§Ã£o', 'representation',
        'diversidade', 'diversity', 'inclusÃ£o', 'inclusion', 'igualdade', 'equality'
    ]
    
    # Verificar se hÃ¡ indicadores positivos
    positive_count = sum(1 for indicator in positive_indicators if indicator in text_lower)
    
    return positive_count > 0

def detect_anatomical_reduction(text):
    """Detecta reduÃ§Ã£o a genitÃ¡lia (sempre hate)"""
    text_lower = text.lower()
    
    # PadrÃµes de reduÃ§Ã£o a genitÃ¡lia
    anatomical_patterns = [
        'homem com buceta', 'mulher com pÃªnis', 'pÃªnis', 'buceta', 'vagina',
        'genitÃ¡lia', 'genital', 'Ã³rgÃ£o sexual', 'parte Ã­ntima',
        'tem que existir', 'deveria ter', 'deveria ser', 'Ã© igual a',
        'Ã© sÃ³', 'nada mais que', 'apenas', 'somente'
    ]
    
    # Verificar se hÃ¡ padrÃµes de reduÃ§Ã£o a genitÃ¡lia
    for pattern in anatomical_patterns:
        if pattern in text_lower:
            return True
    
    return False

def detect_ridicule_context(text):
    """Detecta contexto de ridicularizaÃ§Ã£o (sempre hate)"""
    text_lower = text.lower()
    
    # PadrÃµes de ridicularizaÃ§Ã£o
    ridicule_patterns = [
        'engraÃ§ado', 'engraÃ§ada', 'engraÃ§adÃ­ssimo', 'engraÃ§adÃ­ssima',
        'hilÃ¡rio', 'hilÃ¡ria', 'hilariante', 'cÃ´mico', 'cÃ´mica',
        'ridÃ­culo', 'ridÃ­cula', 'ridicularizar', 'zoar', 'zombar',
        'rir de', 'rindo de', 'risada', 'risadinha', 'piada',
        'brincadeira', 'brincar', 'zoaÃ§Ã£o', 'zoeira',
        'achei engraÃ§ado', 'achei engraÃ§ada', 'engraÃ§ado esse', 'engraÃ§ada esse',
        'nome engraÃ§ado', 'termo engraÃ§ado', 'engraÃ§ado nome', 'engraÃ§ado termo'
    ]
    
    # Verificar se hÃ¡ padrÃµes de ridicularizaÃ§Ã£o
    for pattern in ridicule_patterns:
        if pattern in text_lower:
            return True
    
    return False

def detect_definition_context(text):
    """Detecta contexto de definiÃ§Ã£o/educaÃ§Ã£o (nÃ£o Ã© hate)"""
    text_lower = text.lower()
    
    # PadrÃµes de definiÃ§Ã£o/educaÃ§Ã£o
    definition_patterns = [
        'Ã© uma', 'significa', 'quer dizer', 'definiÃ§Ã£o', 'conceito',
        'explicar', 'entender', 'aprender', 'educar', 'informar',
        'pergunta', 'dÃºvida', 'curiosidade', 'interesse', 'pesquisa',
        'estudo', 'anÃ¡lise', 'discussÃ£o', 'debate', 'conversa',
        'simples', 'simplesmente', 'bÃ¡sico', 'bÃ¡sica', 'fundamental'
    ]
    
    # Verificar se hÃ¡ padrÃµes de definiÃ§Ã£o
    for pattern in definition_patterns:
        if pattern in text_lower:
            return True
    
    return False

def detect_legitimate_question_context(text):
    """Detecta contexto de pergunta legÃ­tima baseado no comprimento e estrutura"""
    text_lower = text.lower()
    
    # Contar palavras
    words = text.split()
    word_count = len(words)
    
    # PadrÃµes de perguntas legÃ­timas
    question_patterns = [
        'pergunta', 'dÃºvida', 'curiosidade', 'interesse', 'pesquisa',
        'entender', 'aprender', 'explicar', 'significa', 'quer dizer',
        'como funciona', 'o que Ã©', 'pode explicar', 'tem como',
        'gostaria de saber', 'queria entender', 'preciso saber'
    ]
    
    # PadrÃµes de cortesia e respeito
    courtesy_patterns = [
        'por favor', 'obrigado', 'obrigada', 'desculpe', 'desculpa',
        'com todo respeito', 'sem ofensa', 'sem hate', 'respeitosamente',
        'educadamente', 'gentilmente', 'cordialmente', 'entendi', 'obrigado pela',
        'obrigada pela', 'valeu', 'brigado', 'brigada'
    ]
    
    # PadrÃµes de hesitaÃ§Ã£o e incerteza
    hesitation_patterns = [
        'acho que', 'creio que', 'talvez', 'possivelmente', 'provavelmente',
        'nÃ£o tenho certeza', 'nÃ£o sei', 'estou confuso', 'confuso',
        'nÃ£o entendi', 'nÃ£o compreendi', 'me explique'
    ]
    
    # Verificar padrÃµes de pergunta legÃ­tima
    has_question_pattern = any(pattern in text_lower for pattern in question_patterns)
    has_courtesy_pattern = any(pattern in text_lower for pattern in courtesy_patterns)
    has_hesitation_pattern = any(pattern in text_lower for pattern in hesitation_patterns)
    
    # Textos longos (>15 palavras) com padrÃµes de respeito/educaÃ§Ã£o
    if word_count > 15 and (has_courtesy_pattern or has_hesitation_pattern):
        return True
    
    # Textos mÃ©dios (6-15 palavras) com padrÃµes de pergunta ou cortesia
    if 6 <= word_count <= 15 and (has_question_pattern or has_courtesy_pattern):
        return True
    
    # Textos muito longos (>25 palavras) - provavelmente elaboraÃ§Ã£o legÃ­tima
    if word_count > 25:
        return True
    
    return False

def detect_short_aggressive_context(text):
    """Detecta contexto de Ã³dio curto e agressivo"""
    text_lower = text.lower()
    
    # Contar palavras
    words = text.split()
    word_count = len(words)
    
    # PadrÃµes de Ã³dio direto e agressivo
    aggressive_patterns = [
        'odeio', 'detesto', 'nojo', 'asco', 'repugnante',
        'nojento', 'escroto', 'desgraÃ§ado', 'arrombado',
        'filho da puta', 'filha da puta', 'merda', 'porra',
        'caralho', 'puta', 'prostituta', 'vagabunda'
    ]
    
    # PadrÃµes de ameaÃ§a e violÃªncia
    threat_patterns = [
        'morrer', 'morra', 'mata', 'matar', 'eliminar',
        'destruir', 'acabar', 'sumir', 'desaparecer'
    ]
    
    # PadrÃµes de rejeiÃ§Ã£o categÃ³rica
    rejection_patterns = [
        'nunca', 'jamais', 'nada', 'zero', 'nunca mais',
        'chega', 'basta', 'suficiente', 'acabou'
    ]
    
    # Verificar padrÃµes agressivos
    has_aggressive_pattern = any(pattern in text_lower for pattern in aggressive_patterns)
    has_threat_pattern = any(pattern in text_lower for pattern in threat_patterns)
    has_rejection_pattern = any(pattern in text_lower for pattern in rejection_patterns)
    
    # Textos curtos (â‰¤8 palavras) com padrÃµes agressivos
    if word_count <= 8 and (has_aggressive_pattern or has_threat_pattern):
        return True
    
    # Textos muito curtos (â‰¤5 palavras) com qualquer padrÃ£o negativo
    if word_count <= 5 and (has_aggressive_pattern or has_threat_pattern or has_rejection_pattern):
        return True
    
    return False

def detect_supportive_emojis(text):
    """Detecta emojis de apoio e suporte (nÃ£o Ã© hate)"""
    # Emojis de coraÃ§Ã£o (apoio)
    heart_emojis = ['â¤ï¸', 'ğŸ§¡', 'ğŸ’›', 'ğŸ’š', 'ğŸ’™', 'ğŸ’œ', 'ğŸ–¤', 'ğŸ¤', 'ğŸ¤', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’˜', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ§¡', 'â¤ï¸']
    
    # Emojis trans e LGBTQIA+ (apoio)
    trans_emojis = ['ğŸ³ï¸â€âš§ï¸', 'ğŸ³ï¸â€ğŸŒˆ', 'ğŸ³ï¸â€âš§ï¸', 'ğŸ³ï¸â€ğŸŒˆ', 'âš§ï¸', 'ğŸ³ï¸â€âš§ï¸']
    
    # Emojis de fogo (apoio, quente)
    fire_emojis = ['ğŸ”¥', 'ğŸŒ¶ï¸', 'ğŸŒ¶ï¸â€ğŸ”¥', 'ğŸ”¥']
    
    # Emojis de apoio geral
    support_emojis = ['ğŸ‘', 'ğŸ™Œ', 'ğŸ’ª', 'âœ¨', 'ğŸŒŸ', 'â­', 'ğŸ’«', 'ğŸ‰', 'ğŸŠ', 'ğŸŒˆ', 'ğŸ¦„']
    
    # Verificar se hÃ¡ emojis de apoio
    for emoji in heart_emojis + trans_emojis + fire_emojis + support_emojis:
        if emoji in text:
            return True
    
    return False

def detect_mocking_emojis(text):
    """Detecta emojis de deboche e ridicularizaÃ§Ã£o - VERSÃƒO MELHORADA"""
    text_lower = text.lower()
    
    # Emojis de deboche especÃ­fico (sempre hate)
    mocking_emojis = ['ğŸ™„', 'ğŸ˜’', 'ğŸ˜¤', 'ğŸ¤¨', 'ğŸ˜‘', 'ğŸ˜', 'ğŸ˜¶', 'ğŸ¤', 'ğŸ˜·', 'ğŸ¤¢', 'ğŸ¤®']
    
    # Emojis de risada (sÃ³ Ã© hate se acompanhado de contexto negativo)
    laugh_emojis = ['ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜†', 'ğŸ˜„', 'ğŸ˜ƒ', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ¤ª', 'ğŸ˜', 'ğŸ˜ˆ']
    
    # Verificar emojis de deboche especÃ­fico
    for emoji in mocking_emojis:
        if emoji in text:
            return True
    
    # Para emojis de risada, verificar contexto
    has_laugh_emoji = any(emoji in text for emoji in laugh_emojis)
    
    if has_laugh_emoji:
        # PadrÃµes de contexto negativo (SÃƒO hate com risada)
        negative_context_patterns = [
            r'\b(viado|bicha|sapatÃ£o|paneleiro|gay|lesbica|bissexual|queer|travesti|trans)\b.*\b(doente|nojento|escroto|desgraÃ§ado|de merda)\b',
            r'\b(que porra|que merda|que bosta|que droga)\b',
            r'\b(desgraÃ§a|desgraÃ§ado|nojento|escroto|filho da puta)\b',
            r'\b(vai se foder|vai tomar no cu|vai pro inferno)\b',
            r'\b(odeio|detesto|repudio|rejeito)\b.*\b(lgbt|gay|lesbica|trans|queer)\b',
            r'\b(palhaÃ§ada|palhaÃ§ade|ridÃ­culo|ridÃ­cula|patÃ©tico|patÃ©tica)\b'
        ]
        
        # Verificar se hÃ¡ contexto negativo
        has_negative_context = any(re.search(pattern, text_lower) for pattern in negative_context_patterns)
        
        if has_negative_context:
            return True
    
    # Se nÃ£o tem emoji, nÃ£o Ã© mocking emoji
    return False

def detect_hate_emojis(text):
    """Detecta emojis de hate e Ã³dio"""
    # Emojis de demÃ´nio (moral religiosa)
    demon_emojis = ['ğŸ˜ˆ', 'ğŸ‘¹', 'ğŸ‘º', 'ğŸ’€', 'â˜ ï¸', 'ğŸ‘»', 'ğŸ­']
    
    # Emojis de vÃ´mito e cocÃ´ (hate)
    disgust_emojis = ['ğŸ¤®', 'ğŸ¤¢', 'ğŸ’©', 'ğŸª£', 'ğŸš½', 'ğŸ§»']
    
    # Emojis de morte e violÃªncia
    violence_emojis = ['ğŸ’€', 'â˜ ï¸', 'ğŸ”ª', 'ğŸ—¡ï¸', 'âš”ï¸', 'ğŸ”«', 'ğŸ’£', 'ğŸ§¨', 'ğŸ’¥', 'ğŸ’¢', 'ğŸ’€']
    
    # Verificar se hÃ¡ emojis de hate
    for emoji in demon_emojis + disgust_emojis + violence_emojis:
        if emoji in text:
            return True
    
    return False

def detect_text_mocking_patterns(text):
    """Detecta padrÃµes de texto que indicam deboche - VERSÃƒO MELHORADA"""
    text_lower = text.lower()
    
    # PadrÃµes de risada em texto (apenas quando acompanhados de contexto negativo)
    laugh_patterns = [
        'kkkk', 'kkkkk', 'kkkkkk', 'kkkkkkk', 'kkkkkkkk',
        'hahaha', 'hahahaha', 'hehehe', 'hehehehe',
        'rsrsrs', 'rsrsrsrs', 'huehue', 'huehuehue',
        'lol', 'lmao', 'rofl', 'hahah', 'heheh'
    ]
    
    # Verificar se hÃ¡ padrÃµes de risada
    has_laugh_pattern = any(pattern in text_lower for pattern in laugh_patterns)
    
    if not has_laugh_pattern:
        return False
    
    # PadrÃµes de contexto negativo (SÃƒO hate com risada)
    negative_context_patterns = [
        r'\b(viado|bicha|sapatÃ£o|paneleiro|gay|lesbica|bissexual|queer|travesti|trans)\b.*\b(doente|nojento|escroto|desgraÃ§ado|de merda)\b',
        r'\b(que porra|que merda|que bosta|que droga)\b',
        r'\b(desgraÃ§a|desgraÃ§ado|nojento|escroto|filho da puta)\b',
        r'\b(vai se foder|vai tomar no cu|vai pro inferno)\b',
        r'\b(odeio|detesto|repudio|rejeito)\b.*\b(lgbt|gay|lesbica|trans|queer)\b',
        r'\b(palhaÃ§ada|palhaÃ§ade|ridÃ­culo|ridÃ­cula|patÃ©tico|patÃ©tica)\b'
    ]
    
    # Verificar se hÃ¡ contexto negativo
    has_negative_context = any(re.search(pattern, text_lower) for pattern in negative_context_patterns)
    
    # SÃ³ Ã© hate se hÃ¡ risada E contexto negativo
    return has_negative_context

def detect_condescending_commands(text):
    """Detecta comandos condescendentes (geralmente hate)"""
    text_lower = text.lower()
    
    # PadrÃµes de comandos condescendentes
    condescending_patterns = [
        'vai estudar', 'vai trabalhar', 'vai trabalhar', 'vai estudar',
        'vai procurar o que fazer', 'vai arrumar o que fazer',
        'vai cuidar da sua vida', 'vai se ocupar',
        'vai ler um livro', 'vai se informar',
        'vai fazer algo Ãºtil', 'vai ser Ãºtil',
        'vai se tratar', 'vai se cuidar',
        'vai procurar ajuda', 'vai se tratar'
    ]
    
    # Verificar se hÃ¡ padrÃµes condescendentes
    for pattern in condescending_patterns:
        if pattern in text_lower:
            return True
    
    return False

def detect_religious_moralism(text):
    """Detecta moralismo religioso (geralmente hate)"""
    text_lower = text.lower()
    
    # Termos religiosos que geralmente indicam moralismo
    religious_terms = [
        'jesus', 'pai', 'deus', 'senhor', 'cristo',
        'sagrado', 'santo', 'abenÃ§oado', 'abenÃ§oada',
        'pecado', 'pecador', 'pecadora', 'condenado', 'condenada',
        'inferno', 'demÃ´nio', 'satanÃ¡s', 'maldito', 'maldita',
        'amaldiÃ§oado', 'amaldiÃ§oada', 'castigo', 'puniÃ§Ã£o',
        'apocalipse', 'revelaÃ§Ã£o', 'profecia', 'bÃ­blia',
        'igreja', 'pastor', 'padre', 'bispo', 'arcebispo'
    ]
    
    # Verificar se hÃ¡ termos religiosos
    for term in religious_terms:
        if term in text_lower:
            return True
    
    return False

def detect_pathologizing_terms(text):
    """Detecta termos que patologizam (geralmente hate)"""
    text_lower = text.lower()
    
    # Termos mÃ©dicos que geralmente patologizam
    pathologizing_terms = [
        'psiquiatra', 'urologista', 'ginecologista', 'psicologo',
        'psicÃ³logo', 'terapeuta', 'analista', 'mÃ©dico',
        'doutor', 'doutora', 'clÃ­nica', 'hospital',
        'tratamento', 'terapia', 'medicamento', 'remÃ©dio',
        'doenÃ§a', 'doente', 'patologia', 'sÃ­ndrome',
        'distÃºrbio', 'transtorno', 'desvio', 'anormalidade'
    ]
    
    # Verificar se hÃ¡ termos patologizantes
    for term in pathologizing_terms:
        if term in text_lower:
            return True
    
    return False

def detect_disguised_hate(text):
    """Detecta hate disfarÃ§ado com pontos, nÃºmeros, etc. - VERSÃƒO MELHORADA"""
    text_lower = text.lower()
    
    # PadrÃµes de hate disfarÃ§ado (apenas casos Ã³bvios de hate)
    disguised_hate_patterns = [
        'ri di cu', 'r1 d1 c0', 'r1 d1 c0', 'r1 d1 c0',
        'viado do caralho', 'bicha nojenta', 'sapatÃ£o desgraÃ§ada', 'paneleiro de merda',
        'gay de merda', 'lÃ©sbica nojenta', 'bissexual escrota', 'queer desgraÃ§ado',
        'travesti nojenta', 'trans doente', 'lgbt de merda', 'lgbtqia escrota'
    ]
    
    # PadrÃµes de contexto positivo (NÃƒO sÃ£o hate)
    positive_context_patterns = [
        r'\b(meu|minha|nosso|nossa)\b.*\b(bar|restaurante|local|lugar|favorito|preferido)\b',
        r'\b(amo|adoro|gosto|aprecio|respeito|apoio|defendo)\b',
        r'\b(orgulho|pride|diversidade|inclusÃ£o|igualdade)\b',
        r'\b(comunidade|grupo|coletivo|movimento)\b',
        r'\b(direitos|direito de ser|vivÃªncia|identidade)\b',
        r'\b(visibilidade|representaÃ§Ã£o|aceitaÃ§Ã£o|tolerÃ¢ncia)\b',
        r'\b(pode sim|pode continuar|uma coisa n impede|nÃ£o impede)\b',
        r'\b(diagnÃ³stico|abriu|inclusive|correlato)\b',
        r'\b(entendi|entendendo|compreendo|compreendendo)\b',
        r'\b(sapatÃ£o|gay|lesbica|bissexual|queer|travesti|trans)\b.*\b(favorito|preferido|legal|bom|Ã³timo)\b',
        r'\b(bar|restaurante|local|lugar)\b.*\b(sapatÃ£o|gay|lesbica|bissexual|queer|travesti|trans)\b'
    ]
    
    # Verificar se hÃ¡ padrÃµes de hate disfarÃ§ado
    has_disguised_hate = any(pattern in text_lower for pattern in disguised_hate_patterns)
    
    # Verificar se hÃ¡ contexto positivo
    has_positive_context = any(re.search(pattern, text_lower) for pattern in positive_context_patterns)
    
    # Se tem contexto positivo, NÃƒO Ã© hate
    if has_positive_context:
        return False
    
    # Se tem padrÃ£o de hate disfarÃ§ado, Ã© hate
    if has_disguised_hate:
        return True
    
    # Verificar termos LGBTQIA+ isolados (sem contexto positivo)
    lgbtqia_terms = ['viado', 'bicha', 'sapatÃ£o', 'paneleiro', 'gay', 'lÃ©sbica', 'bissexual', 'queer', 'travesti', 'trans', 'lgbt', 'lgbtqia']
    
    # Contar quantos termos LGBTQIA+ existem
    lgbtqia_count = sum(1 for term in lgbtqia_terms if term in text_lower)
    
    # Se hÃ¡ muitos termos LGBTQIA+ sem contexto positivo, pode ser hate
    if lgbtqia_count >= 2:
        return True
    
    # Se hÃ¡ apenas 1 termo LGBTQIA+ sem contexto positivo, verificar se Ã© usado de forma negativa
    if lgbtqia_count == 1:
        # PadrÃµes que indicam uso negativo
        negative_patterns = [
            r'\b(odeio|detesto|nojento|repugnante|asqueroso)\b',
            r'\b(doente|doenÃ§a|tratamento|cura|psicolÃ³gico|mental)\b',
            r'\b(pecado|deus|demÃ´nio|igreja|bÃ­blia|cristÃ£o)\b',
            r'\b(natural|normal|anormal|aberraÃ§Ã£o|erro)\b',
            r'\b(filho da puta|filha da puta|arrombado|escroto|desgraÃ§ado)\b'
        ]
        
        # Se hÃ¡ padrÃµes negativos, Ã© hate
        if any(re.search(pattern, text_lower) for pattern in negative_patterns):
            return True
    
    return False

def detect_shame_terms(text):
    """Detecta termos de vergonha (geralmente hate)"""
    text_lower = text.lower()
    
    # Termos de vergonha
    shame_terms = [
        'vergonha', 'vergonhoso', 'vergonhosa', 'vergonhoso',
        'envergonhado', 'envergonhada', 'envergonhado',
        'sem vergonha', 'sem-vergonha', 'semvergonha',
        'desvergonhado', 'desvergonhada', 'desvergonhado',
        'atrevido', 'atrevida', 'atrevido',
        'ousado', 'ousada', 'ousado'
    ]
    
    # Verificar se hÃ¡ termos de vergonha
    for term in shame_terms:
        if term in text_lower:
            return True
    
    return False

def detect_curse_words(text):
    """Detecta palavrÃµes (geralmente hate)"""
    text_lower = text.lower()
    
    # PalavrÃµes
    curse_words = [
        'bosta', 'merda', 'porra', 'caralho', 'puta',
        'filho da puta', 'filha da puta', 'arrombado',
        'arrombada', 'escroto', 'escrota', 'nojento',
        'nojenta', 'desgraÃ§ado', 'desgraÃ§ada', 'lixo',
        'lixÃ£o', 'sujo', 'suja', 'fedido', 'fedida'
    ]
    
    # Verificar se hÃ¡ palavrÃµes
    for word in curse_words:
        if word in text_lower:
            return True
    
    return False

def detect_misogynistic_terms(text):
    """Detecta termos machistas (geralmente hate)"""
    text_lower = text.lower()
    
    # Termos machistas
    misogynistic_terms = [
        'lavar louÃ§a', 'vai lavar louÃ§a', 'cozinha', 'vai cozinhar',
        'roupa', 'vai passar roupa', 'limpeza', 'vai limpar',
        'casa', 'vai cuidar da casa', 'filhos', 'vai cuidar dos filhos',
        'mulher', 'sua mulher', 'esposa', 'sua esposa',
        'mÃ£e', 'sua mÃ£e', 'avÃ³', 'sua avÃ³'
    ]
    
    # Verificar se hÃ¡ termos machistas
    for term in misogynistic_terms:
        if term in text_lower:
            return True
    
    return False

def detect_condescending_metaphors(text):
    """Detecta metÃ¡foras condescendentes (geralmente hate)"""
    text_lower = text.lower()
    
    # MetÃ¡foras condescendentes
    condescending_metaphors = [
        'um lote', 'capinar um lote', 'vai capinar um lote',
        'plantar', 'vai plantar', 'semeiar', 'vai semear',
        'colher', 'vai colher', 'cavar', 'vai cavar',
        'construir', 'vai construir', 'trabalhar', 'vai trabalhar',
        'servir', 'vai servir', 'obedecer', 'vai obedecer'
    ]
    
    # Verificar se hÃ¡ metÃ¡foras condescendentes
    for metaphor in condescending_metaphors:
        if metaphor in text_lower:
            return True
    
    return False

def detect_condescending_insults(text):
    """Detecta insultos condescendentes (geralmente hate)"""
    text_lower = text.lower()
    
    # Insultos condescendentes
    condescending_insults = [
        'desempregado', 'desempregada', 'vagabundo', 'vagabunda',
        'preguiÃ§oso', 'preguiÃ§osa', 'inÃºtil', 'inÃºtil',
        'burro', 'burra', 'idiota', 'imbecil',
        'estÃºpido', 'estÃºpida', 'estupidez', 'burrice',
        'ignorante', 'analfabeto', 'analfabeta', 'inculto', 'inculta'
    ]
    
    # Verificar se hÃ¡ insultos condescendentes
    for insult in condescending_insults:
        if insult in text_lower:
            return True
    
    return False

def detect_excessive_punctuation(text):
    """Detecta excessos de pontuaÃ§Ã£o (geralmente hate) - VERSÃƒO MELHORADA"""
    text_lower = text.lower()
    
    # Contar exclamaÃ§Ãµes e interrogaÃ§Ãµes
    exclamation_count = text.count('!')
    question_count = text.count('?')
    
    # PadrÃµes de contexto positivo (NÃƒO sÃ£o hate mesmo com pontuaÃ§Ã£o excessiva)
    positive_context_patterns = [
        r'\b(que legal|que bom|que Ã³timo|que incrÃ­vel|que maravilhoso)\b',
        r'\b(parabÃ©ns|parabÃ©ns|felicitaÃ§Ãµes|congratulations)\b',
        r'\b(amo|adoro|gosto|aprecio|respeito|apoio|defendo)\b',
        r'\b(orgulho|pride|diversidade|inclusÃ£o|igualdade)\b',
        r'\b(conforto|tranquilidade|paz|alegria|felicidade)\b',
        r'\b(meu amor|minha amor|amor)\b',
        r'\b(seja feliz|feliz sempre|seja o que vocÃª quiser)\b',
        r'\b(obrigada|obrigado|thanks|thank you)\b',
        r'\b(incrÃ­vel|maravilhoso|fantÃ¡stico|Ã³timo|excelente)\b',
        r'\b(amei|adoro|gostei|curti|aprovei)\b'
    ]
    
    # PadrÃµes de contexto negativo (SÃƒO hate com pontuaÃ§Ã£o excessiva)
    negative_context_patterns = [
        r'\b(viado|bicha|sapatÃ£o|paneleiro|gay|lesbica|bissexual|queer|travesti|trans)\b.*\b(doente|nojento|escroto|desgraÃ§ado|de merda)\b',
        r'\b(que porra|que merda|que bosta|que droga)\b',
        r'\b(desgraÃ§a|desgraÃ§ado|nojento|escroto|filho da puta)\b',
        r'\b(vai se foder|vai tomar no cu|vai pro inferno)\b',
        r'\b(odeio|detesto|repudio|rejeito)\b.*\b(lgbt|gay|lesbica|trans|queer)\b'
    ]
    
    # Verificar se hÃ¡ contexto positivo
    has_positive_context = any(re.search(pattern, text_lower) for pattern in positive_context_patterns)
    
    # Verificar se hÃ¡ contexto negativo
    has_negative_context = any(re.search(pattern, text_lower) for pattern in negative_context_patterns)
    
    # Se tem contexto positivo, NÃƒO Ã© hate
    if has_positive_context:
        return False
    
    # Se tem contexto negativo, Ã© hate
    if has_negative_context:
            return True
    
    # SÃ³ considerar hate se hÃ¡ pontuaÃ§Ã£o excessiva E contexto negativo
    # PontuaÃ§Ã£o excessiva sozinha nÃ£o Ã© hate
    return False

def detect_direct_insults(text):
    """Detecta insultos diretos (geralmente hate)"""
    text_lower = text.lower()
    
    # Insultos diretos
    direct_insults = [
        'patÃ©tico', 'patÃ©tica', 'ridÃ­culo', 'ridÃ­cula',
        'nojento', 'nojenta', 'repugnante', 'asqueroso', 'asquerosa',
        'desprezÃ­vel', 'vergonhoso', 'vergonhosa', 'humilhante',
        'ofensivo', 'ofensiva', 'agressivo', 'agressiva',
        'violento', 'violenta', 'brutal', 'cruel'
    ]
    
    # Verificar se hÃ¡ insultos diretos
    for insult in direct_insults:
        if insult in text_lower:
            return True
    
    return False

def detect_negative_context(text):
    """Detecta contexto negativo para termos LGBTQIA+"""
    text_lower = text.lower()
    
    # Palavras negativas que indicam contexto de Ã³dio/rejeiÃ§Ã£o
    negative_indicators = [
        'Ã³dio', 'hate', 'nojo', 'disgust', 'repugnante', 'repugnant',
        'nojento', 'disgusting', 'escroto', 'disgusting', 'desgraÃ§ado', 'damned',
        'arrombado', 'fucked', 'merda', 'shit', 'caralho', 'fuck',
        'filho da puta', 'son of a bitch', 'filha da puta', 'daughter of a bitch',
        'doente', 'sick', 'anormal', 'abnormal', 'errado', 'wrong',
        'pecado', 'sin', 'demÃ´nio', 'devil', 'inferno', 'hell',
        'morte', 'death', 'morrer', 'die', 'matar', 'kill',
        'eliminar', 'eliminate', 'destruir', 'destroy', 'acabar',
        # Adicionar mais indicadores negativos especÃ­ficos
        'nojenta', 'escrota', 'desgraÃ§ada', 'arrombada', 'merdosa', 'caralhosa',
        'filha da puta', 'puta', 'prostituta', 'vagabunda', 'safada',
        'doenÃ§a', 'doente', 'anormal', 'errado', 'pecado', 'demÃ´nio',
        'inferno', 'morte', 'morrer', 'matar', 'eliminar', 'destruir'
    ]
    
    # Verificar se hÃ¡ indicadores negativos
    negative_count = sum(1 for indicator in negative_indicators if indicator in text_lower)
    
    return negative_count > 0

def contextual_gender_dissidence_rule(text):
    """Regra contextual para termos de dissidÃªncia de gÃªnero"""
    text_lower = text.lower()
    
    # Termos de dissidÃªncia de gÃªnero que podem ser positivos ou negativos
    gender_dissidence_terms = [
        'boyceta', 'boycet', 'sapatÃ£o', 'travesti', 'transgÃªnero', 'transgenero',
        'nÃ£o-binÃ¡rio', 'nao-binario', 'genderqueer', 'queer',
        'drag queen', 'drag king', 'crossdresser'
    ]
    
    # Verificar se hÃ¡ termos de dissidÃªncia de gÃªnero
    has_gender_term = any(term in text_lower for term in gender_dissidence_terms)
    
    if not has_gender_term:
        return None
    
    # 1. PRIMEIRO: Verificar emojis de hate (sempre hate)
    # Esta tem prioridade mÃ¡xima para detectar Ã³dio explÃ­cito
    if detect_hate_emojis(text):
        return "hate"
    
    # 2. SEGUNDO: Verificar emojis de apoio (nÃ£o Ã© hate)
    # Esta tem prioridade alta para proteger apoio legÃ­timo
    if detect_supportive_emojis(text):
        return "nÃ£o_hate"
    
    # 3. TERCEIRO: Verificar contexto de pergunta legÃ­tima (nÃ£o Ã© hate)
    if detect_legitimate_question_context(text):
        return "nÃ£o_hate"
    
    # 4. QUARTO: Verificar contexto de definiÃ§Ã£o/educaÃ§Ã£o (nÃ£o Ã© hate)
    if detect_definition_context(text):
        return "nÃ£o_hate"
    
    # 5. QUINTO: Verificar Ã³dio curto e agressivo (sempre hate)
    if detect_short_aggressive_context(text):
        return "hate"
    
    # 6. SEXTO: Verificar ridicularizaÃ§Ã£o (sempre hate)
    if detect_ridicule_context(text):
        return "hate"
    
    # 7. SÃ‰TIMO: Verificar reduÃ§Ã£o a genitÃ¡lia (sempre hate)
    if detect_anatomical_reduction(text):
        return "hate"
    
    # 8. OITAVO: Verificar emojis de deboche (possÃ­vel hate)
    if detect_mocking_emojis(text) or detect_text_mocking_patterns(text):
        return "hate"
    
    # 9. NONO: Detectar contexto positivo/negativo
    is_positive = detect_positive_context(text)
    is_negative = detect_negative_context(text)
    
    # Se hÃ¡ contexto positivo e nÃ£o hÃ¡ contexto negativo â†’ nÃ£o Ã© hate
    if is_positive and not is_negative:
        return "nÃ£o_hate"
    
    # Se hÃ¡ contexto negativo â†’ Ã© hate
    if is_negative:
        return "hate"
    
    # Se nÃ£o hÃ¡ contexto claro, usar regras especÃ­ficas
    return None

def specific_gender_terms_rule(text):
    """Regra especÃ­fica para termos de gÃªnero problemÃ¡ticos"""
    text_lower = text.lower()
    
    # Termos especÃ­ficos que precisam de anÃ¡lise cuidadosa
    problematic_terms = {
        'boyceta': {
            'positive_contexts': ['orgulho', 'beleza', 'identidade', 'expressÃ£o'],
            'negative_contexts': ['nojento', 'escroto', 'desgraÃ§ado', 'arrombado']
        },
        'sapatÃ£o': {
            'positive_contexts': ['orgulho', 'forÃ§a', 'coragem', 'identidade'],
            'negative_contexts': ['nojento', 'escroto', 'desgraÃ§ado', 'arrombado']
        },
        'travesti': {
            'positive_contexts': ['orgulho', 'beleza', 'identidade', 'expressÃ£o'],
            'negative_contexts': ['nojento', 'escroto', 'desgraÃ§ado', 'arrombado']
        }
    }
    
    for term, contexts in problematic_terms.items():
        if term in text_lower:
            # Verificar contexto positivo
            positive_found = any(ctx in text_lower for ctx in contexts['positive_contexts'])
            # Verificar contexto negativo
            negative_found = any(ctx in text_lower for ctx in contexts['negative_contexts'])
            
            # Se hÃ¡ contexto positivo e nÃ£o hÃ¡ negativo â†’ nÃ£o Ã© hate
            if positive_found and not negative_found:
                return "nÃ£o_hate"
            
            # Se hÃ¡ contexto negativo â†’ Ã© hate
            if negative_found:
                return "hate"
    
    return None

def enhanced_hybrid_rules(text):
    """Regras hÃ­bridas aprimoradas com contexto de gÃªnero"""
    
    # 1. Primeiro: verificar contexto de dissidÃªncia de gÃªnero
    gender_result = contextual_gender_dissidence_rule(text)
    if gender_result:
        return gender_result
    
    # 2. Segundo: verificar termos especÃ­ficos problemÃ¡ticos
    specific_result = specific_gender_terms_rule(text)
    if specific_result:
        return specific_result
    
    # 3. Terceiro: aplicar regras originais de insulto
    text_lower = text.lower()
    
    # PadrÃ£o: [termo LGBTQIA+] + [insulto]
    insult_patterns = [
        'filho da puta', 'filha da puta', 'arrombado', 'arrombada',
        'escroto', 'escrota', 'nojento', 'nojenta', 'desgraÃ§ado', 'desgraÃ§ada',
        'de merda', 'do caralho', 'filho da puta', 'filha da puta'
    ]
    
    lgbtqia_terms = ['viado', 'bicha', 'gay', 'lÃ©sbica', 'bissexual', 'queer']
    
    # Se hÃ¡ termo LGBTQIA+ + insulto â†’ FORÃ‡AR assÃ©dio
    if any(term in text_lower for term in lgbtqia_terms) and \
       any(insult in text_lower for insult in insult_patterns):
        return "assedio_insulto"
    
    return None

# --- FunÃ§Ã£o para Detectar Falsos Positivos ---
def has_positive_adjective(text):
    """Verifica se o texto contÃ©m adjetivos positivos"""
    positive_adjectives = [
        'delÃ­cia', 'maravilhoso', 'lindo', 'bonito', 'incrÃ­vel', 'fantÃ¡stico',
        'perfeito', 'Ã³timo', 'excelente', 'magnÃ­fico', 'esplÃªndido', 'formidÃ¡vel',
        'adorÃ¡vel', 'encantador', 'fabuloso', 'sensacional', 'extraordinÃ¡rio',
        'divino', 'celestial', 'majestoso', 'sublime', 'extraordinÃ¡rio'
    ]
    
    normalized = normalize_text(text)
    return any(adj in normalized for adj in positive_adjectives)

def is_lgbtqia_pattern(text):
    """Verifica se segue o padrÃ£o 'ser [termo LGBTQIA+] Ã© [adjetivo]'"""
    lgbtqia_terms = [
        'gay', 'lÃ©sbica', 'trans', 'bicha', 'viado', 'sapatÃ£o', 'paneleiro', 'paneleira',
        'travesti', 'lgbt', 'lgbtqia', 'queer', 'homossexual', 'bissexual', 'pansexual',
        'assexual', 'nÃ£o-binÃ¡rio', 'intersexo', 'transgÃªnero', 'transexual'
    ]
    
    normalized = normalize_text(text)
    
    # PadrÃ£o: ser [termo] Ã© [algo]
    pattern = r'ser\s+(\w+)\s+Ã©\s+(.+)'
    match = re.search(pattern, normalized)
    
    if match:
        term = match.group(1)
        return term in lgbtqia_terms
    
    return False

# --- Carregamento dos Modelos Reais ---
print("ğŸ”„ Carregando modelos reais...")

try:
    # Carregar modelo binÃ¡rio (usando subpasta)
    print("ğŸ“¦ Carregando modelo binÃ¡rio...")
    tokenizer_binary = AutoTokenizer.from_pretrained(MODEL_PATH, subfolder="model-binary-expanded-with-toldbr")
    model_binary = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, subfolder="model-binary-expanded-with-toldbr")
    
    # Carregar modelo especializado (usando subpasta)
    print("ğŸ“¦ Carregando modelo especializado...")
    tokenizer_specialized = AutoTokenizer.from_pretrained(MODEL_PATH, subfolder="model-specialized-expanded")
    model_specialized = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, subfolder="model-specialized-expanded")
    
    print("âœ… Modelos ensemble corretos carregados com sucesso!")
    
except Exception as e:
    print(f"âš ï¸ Erro ao carregar modelos: {e}")
    print("ğŸ”„ Usando sistema de fallback...")
    
    # Fallback para sistema de palavras-chave
    def simulate_hate_detection(text):
        text_lower = text.lower()
        lgbtqia_words = ['gay', 'lÃ©sbica', 'bicha', 'viado', 'sapatÃ£o', 'paneleiro', 'paneleira', 
                         'travesti', 'trans', 'lgbt', 'lgbtqia', 'queer', 'faggot', 'dyke', 'tranny']
        hate_words = ['morrer', 'morra', 'mata', 'matar', 'odeio', 'odeia', 'detesto', 'detesta',
                      'vergonha', 'nojo', 'asco', 'repugnante', 'nojento', 'abominÃ¡vel',
                      'odio', 'Ã³dio', 'lixo', 'desgraÃ§a', 'maldito', 'anormal', 'doente']
        insult_words = ['merda', 'porra', 'caralho', 'puta', 'filho da puta', 'desgraÃ§a', 
                       'escÃ³ria', 'nojento', 'abominaÃ§Ã£o', 'vergonha', 'doenÃ§a']
        religious_words = ['pecado', 'pecador', 'condenado', 'inferno', 'demÃ´nio', 'satanÃ¡s', 
                          'maldito', 'amaldiÃ§oado']
        
        hate_patterns = [
            lambda t: any(word in t for word in lgbtqia_words) and any(phrase in t for phrase in ['deveria morrer', 'deveria morre', 'deveria morr', 'deveria mor', 'deveria mo', 'deveria m', 'deveria']),
            lambda t: any(word in t for word in lgbtqia_words) and any(word in t for word in ['de merda', 'merda']),
            lambda t: any(word in t for word in lgbtqia_words) and any(word in t for word in ['Ã© pecado', 'pecado', 'pecador']),
            lambda t: any(word in t for word in hate_words),
            lambda t: any(word in t for word in lgbtqia_words) and any(word in t for word in insult_words),
            lambda t: any(word in t for word in lgbtqia_words) and any(word in t for word in religious_words),
        ]
        
        is_hate = False
        hate_prob = 0.1
        specialized_class = "N/A"
        
        for i, pattern in enumerate(hate_patterns):
            if pattern(text_lower):
                is_hate = True
                hate_prob = min(0.7 + (i * 0.05), 0.95)
                if i == 0:
                    specialized_class = "AmeaÃ§a/ViolÃªncia"
                elif i == 1:
                    specialized_class = "AssÃ©dio/Insulto"
                elif i == 2:
                    specialized_class = "Ã“dio Religioso"
                elif any(word in text_lower for word in ['trans', 'travesti', 'tranny']):
                    specialized_class = "Transfobia"
                else:
                    specialized_class = "AssÃ©dio/Insulto"
                break
        
        if not is_hate:
            lgbtqia_count = sum(1 for word in lgbtqia_words if word in text_lower)
            hate_count = sum(1 for word in hate_words if word in text_lower)
            insult_count = sum(1 for word in insult_words if word in text_lower)
            if lgbtqia_count > 0 and (hate_count > 0 or insult_count > 0):
                is_hate = True
                hate_prob = min(0.6 + (lgbtqia_count + hate_count + insult_count) * 0.1, 0.9)
                specialized_class = "AssÃ©dio/Insulto"
        
        return {
            'is_hate': is_hate,
            'hate_probability': hate_prob,
            'specialized_class': specialized_class,
            'confidence': max(hate_prob, 1-hate_prob)
        }

# --- FunÃ§Ã£o de PrediÃ§Ã£o com Regras Contextuais ---
def detect_positive_context_with_emojis(text):
    """Detecta contexto positivo com emojis de apoio"""
    text_lower = text.lower()
    
    # Emojis de apoio e positividade
    positive_emojis = ['â¤ï¸', 'ğŸ’–', 'ğŸ’•', 'ğŸ’—', 'ğŸ’', 'ğŸ’˜', 'ğŸ’', 'ğŸ’Ÿ', 'â™¥ï¸', 'ğŸ’œ', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ§¡', 'ğŸ¤', 'ğŸ–¤', 'ğŸ¤', 'ğŸ’¯', 'âœ¨', 'ğŸŒŸ', 'â­', 'ğŸ’«', 'ğŸŒˆ', 'ğŸ¦„', 'ğŸ‘', 'ğŸ™Œ', 'ğŸ‘', 'ğŸ‘Œ', 'ğŸ¤', 'ğŸ¤—', 'ğŸ¤²', 'ğŸ™', 'ğŸ’ª', 'ğŸ‰', 'ğŸŠ', 'ğŸˆ', 'ğŸ', 'ğŸ†', 'ğŸ¥‡', 'ğŸ¥°', 'ğŸ˜', 'ğŸ¥º', 'ğŸ˜Š', 'ğŸ˜‡', 'ğŸ˜Œ', 'ğŸ˜‹', 'ğŸ¤¤', 'ğŸ˜˜', 'ğŸ˜—', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ˜¸', 'ğŸ˜¹', 'ğŸ˜º', 'ğŸ˜»', 'ğŸ˜¼', 'ğŸ˜½', 'ğŸ™€', 'ğŸ˜¿', 'ğŸ˜¾']
    
    # PadrÃµes de positividade
    positive_patterns = [
        r'\b(obrigada|obrigado|obrigad[ao])\b',
        r'\b(amo|adoro|gosto|aprecio|respeito|apoio|defendo)\b',
        r'\b(orgulho|pride|diversidade|inclusÃ£o|igualdade)\b',
        r'\b(conforto|tranquilidade|paz|alegria|felicidade)\b',
        r'\b(nÃ£o tÃ´ sozinha|nÃ£o estou sozinha|nÃ£o estou sozinho|nÃ£o tÃ´ sozinho)\b'
    ]
    
    # Verificar emojis positivos
    has_positive_emoji = any(emoji in text for emoji in positive_emojis)
    
    # Verificar padrÃµes positivos
    has_positive_pattern = any(re.search(pattern, text_lower) for pattern in positive_patterns)
    
    return has_positive_emoji and has_positive_pattern

def detect_neutral_language_only(text):
    """Detecta se Ã© apenas linguagem neutra sozinha (NÃƒO Ã© hate)"""
    text_lower = text.lower().strip()
    
    # Palavras de linguagem neutra sozinhas
    neutral_words = ['todes', 'lules', 'mussum', 'elu', 'delu', 'nelu', 'aquelu', 'daquelu']
    
    # Verificar se Ã© apenas uma palavra de linguagem neutra
    if text_lower in neutral_words:
        return True
    
    # Verificar se sÃ£o apenas palavras de linguagem neutra separadas por espaÃ§o
    words = text_lower.split()
    if len(words) <= 3 and all(word in neutral_words for word in words):
        return True
    
    return False

def detect_single_emoji_context(text):
    """Detecta se Ã© apenas um emoji sozinho ou com contexto mÃ­nimo"""
    text_stripped = text.strip()
    
    # Emojis que sozinhos nÃ£o devem ser hate (apenas neutros)
    neutral_single_emojis = ['ğŸ˜‘', 'ğŸ˜', 'ğŸ˜¶', 'ğŸ¤', 'ğŸ˜·', 'ğŸ¤”', 'ğŸ˜•', 'ğŸ˜Ÿ', 'ğŸ˜”', 'ğŸ˜', 'ğŸ˜¢', 'ğŸ˜­']
    
    # Emojis de hate que NÃƒO devem ser considerados neutros
    hate_emojis = ['ğŸ˜¤', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ¤¬', 'ğŸ˜ˆ', 'ğŸ‘¿', '0', 'â˜ ï¸', 'ğŸ‘»']
    
    # Se tem emoji de hate, NÃƒO Ã© contexto neutro
    if any(emoji in text for emoji in hate_emojis):
        return False
    
    # Se Ã© apenas um emoji neutro
    if len(text_stripped) <= 3 and any(emoji in text_stripped for emoji in neutral_single_emojis):
        return True
    
    # Se Ã© emoji neutro + texto muito curto (ex: "ğŸ˜‘" ou "O Ã³bvio precisa ser dito ğŸ˜‘")
    if len(text_stripped) <= 50 and any(emoji in text_stripped for emoji in neutral_single_emojis):
        return True
    
    return False

def detect_orgulho_lgbtqia(text):
    """Detecta padrÃµes de orgulho LGBTQIA+"""
    text_lower = text.lower()
    
    # PadrÃµes de orgulho LGBTQIA+
    orgulho_patterns = [
        r'\b(com muito orgulho|com orgulho|sou orgulhoso|sou orgulhosa)\b',
        r'\b(me orgulho|orgulho de ser|orgulho de mim|orgulho da minha)\b',
        r'\b(sou sapatÃ£o|sou gay|sou lÃ©sbica|sou bissexual|sou queer)\b',
        r'\b(sou trans|sou travesti|sou transgÃªnero|sou transgenero)\b',
        r'\b(orgulho lgbt|orgulho lgbtqia|pride|diversidade)\b'
    ]
    
    return any(re.search(pattern, text_lower) for pattern in orgulho_patterns)

def detect_respeito_aceitacao(text):
    """Detecta padrÃµes de respeito e aceitaÃ§Ã£o"""
    text_lower = text.lower()
    
    # PadrÃµes de respeito e aceitaÃ§Ã£o
    respeito_patterns = [
        r'\b(respeitar|respeito|aceitar|aceitaÃ§Ã£o|tolerÃ¢ncia)\b',
        r'\b(diversidade|inclusÃ£o|igualdade|direitos|direito de ser)\b',
        r'\b(vivÃªncia pessoal|vivÃªncia deve ser respeitada)\b',
        r'\b(empatia e o respeito|respeito nÃ£o pode ser seletivos)\b',
        r'\b(promover um debate de respeito)\b'
    ]
    
    return any(re.search(pattern, text_lower) for pattern in respeito_patterns)

def detect_curse_words_positive_context(text):
    """Detecta palavrÃµes em contexto positivo"""
    text_lower = text.lower()
    
    # PalavrÃµes comuns
    curse_words = ['caralho', 'porra', 'merda', 'bosta', 'puta', 'foda']
    
    # PadrÃµes de contexto positivo
    positive_context_patterns = [
        r'\b(obrigada|obrigado|obrigad[ao])\b',
        r'\b(conforto|tranquilidade|paz|alegria|felicidade)\b',
        r'\b(nÃ£o tÃ´ sozinha|nÃ£o estou sozinha|nÃ£o estou sozinho|nÃ£o tÃ´ sozinho)\b',
        r'\b(gente|pessoas|amigos|amigas)\b'
    ]
    
    has_curse_word = any(curse in text_lower for curse in curse_words)
    has_positive_context = any(re.search(pattern, text_lower) for pattern in positive_context_patterns)
    
    return has_curse_word and has_positive_context

def detect_respeito_boyceta(text):
    """Detecta padrÃµes de respeito com 'boyceta'"""
    text_lower = text.lower()
    
    # PadrÃµes de respeito com boyceta
    respeito_patterns = [
        r'\b(respeita|respeito|respeitem)\b.*\b(boyceta|boycetas)\b',
        r'\b(boyceta|boycetas)\b.*\b(respeita|respeito|respeitem)\b'
    ]
    
    return any(re.search(pattern, text_lower) for pattern in respeito_patterns)

def detect_hate_emojis_with_laughter(text):
    """Detecta emojis de hate com risadas"""
    # Emojis de hate
    hate_emojis = ['ğŸ‘¿', 'ğŸ˜ˆ', 'ğŸ’€', 'â˜ ï¸', 'ğŸ‘»', 'ğŸ¤¬', 'ğŸ˜¡', 'ğŸ˜ ']
    
    # Emojis de risada
    laugh_emojis = ['ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜†', 'ğŸ˜„', 'ğŸ˜ƒ', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ¤ª']
    
    has_hate_emoji = any(emoji in text for emoji in hate_emojis)
    has_laugh_emoji = any(emoji in text for emoji in laugh_emojis)
    
    return has_hate_emoji and has_laugh_emoji

def detect_palhacada_with_laughter(text):
    """Detecta palhaÃ§ada com risadas (hate contra linguagem neutra)"""
    text_lower = text.lower()
    
    # PadrÃµes de palhaÃ§ada
    palhacada_patterns = [
        r'\b(palhaÃ§ada|palhacada|palhaÃ§ade)\b',
        r'\b(pare de|para de|chega de)\b.*\b(palhaÃ§ada|palhacada|palhaÃ§ade)\b'
    ]
    
    # Emojis de risada
    laugh_emojis = ['ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜†', 'ğŸ˜„', 'ğŸ˜ƒ', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ¤ª']
    laugh_text = ['kkkk', 'haha', 'hehe', 'rsrs']
    
    has_palhacada = any(re.search(pattern, text_lower) for pattern in palhacada_patterns)
    has_laugh = any(emoji in text for emoji in laugh_emojis) or any(laugh in text_lower for laugh in laugh_text)
    
    return has_palhacada and has_laugh

def detect_ciphered_curse_words(text):
    """Detecta palavrÃµes cifrados com nÃºmeros"""
    text_lower = text.lower()
    
    # PadrÃµes de palavrÃµes cifrados
    ciphered_patterns = [
        r'\bp0rr4\b', r'\bcar4lh0\b', r'\bm3rd4\b', r'\bb0st4\b',
        r'\bput4\b', r'\bf0d4\b', r'\bvi4d0\b', r'\bb1ch4\b'
    ]
    
    return any(re.search(pattern, text_lower) for pattern in ciphered_patterns)

def detect_pathologizing_with_laughter(text):
    """Detecta termos patologizantes com risadas"""
    text_lower = text.lower()
    
    # Termos patologizantes
    pathologizing_terms = [
        r'\b(laudo|diagnÃ³stico|psiquiÃ¡trico|psicolÃ³gico|mental)\b',
        r'\b(doente|doenÃ§a|tratamento|cura|terapia)\b',
        r'\b(distÃºrbio|sÃ­ndrome|transtorno|patologia)\b'
    ]
    
    # Emojis de risada
    laugh_emojis = ['ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜†', 'ğŸ˜„', 'ğŸ˜ƒ', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ¤ª']
    laugh_text = ['kkkk', 'haha', 'hehe', 'rsrs']
    
    has_pathologizing = any(re.search(pattern, text_lower) for pattern in pathologizing_terms)
    has_laugh = any(emoji in text for emoji in laugh_emojis) or any(laugh in text_lower for laugh in laugh_text)
    
    return has_pathologizing and has_laugh

def detect_positive_emojis_only(text):
    """Detecta apenas emojis positivos (nÃ£o sÃ£o hate)"""
    # Emojis positivos
    positive_emojis = ['ğŸ˜', 'ğŸ¥°', 'ğŸ˜˜', 'ğŸ˜—', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ˜¸', 'ğŸ˜¹', 'ğŸ˜º', 'ğŸ˜»', 'ğŸ˜¼', 'ğŸ˜½', 'ğŸ™€', 'ğŸ˜¿', 'ğŸ˜¾', 'â¤ï¸', 'ğŸ’–', 'ğŸ’•', 'ğŸ’—', 'ğŸ’', 'ğŸ’˜', 'ğŸ’', 'ğŸ’Ÿ', 'â™¥ï¸', 'ğŸ’œ', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ§¡', 'ğŸ¤', 'ğŸ–¤', 'ğŸ¤', 'ğŸ’¯', 'âœ¨', 'ğŸŒŸ', 'â­', 'ğŸ’«', 'ğŸŒˆ', 'ğŸ¦„', 'ğŸ‘', 'ğŸ™Œ', 'ğŸ‘', 'ğŸ‘Œ', 'ğŸ¤', 'ğŸ¤—', 'ğŸ¤²', 'ğŸ™', 'ğŸ’ª', 'ğŸ‰', 'ğŸŠ', 'ğŸˆ', 'ğŸ', 'ğŸ†', 'ğŸ¥‡']
    
    # Verificar se o texto Ã© apenas emojis positivos
    text_stripped = text.strip()
    
    # Se Ã© apenas emojis positivos
    if all(char in positive_emojis or char.isspace() for char in text_stripped):
        return True
    
    return False

def detect_positive_context_with_punctuation(text):
    """Detecta contexto positivo com pontuaÃ§Ã£o excessiva"""
    text_lower = text.lower()
    
    # PadrÃµes de contexto positivo
    positive_patterns = [
        r'\b(meu amor|minha amor|amor)\b',
        r'\b(seja o que vocÃª quiser|seja feliz|feliz sempre)\b',
        r'\b(amo|adoro|gosto|aprecio|respeito|apoio|defendo)\b',
        r'\b(orgulho|pride|diversidade|inclusÃ£o|igualdade)\b',
        r'\b(conforto|tranquilidade|paz|alegria|felicidade)\b'
    ]
    
    # Verificar se hÃ¡ padrÃµes positivos
    has_positive_pattern = any(re.search(pattern, text_lower) for pattern in positive_patterns)
    
    # Verificar se hÃ¡ pontuaÃ§Ã£o excessiva
    has_excessive_punctuation = bool(re.search(r'[!?]{3,}', text))
    
    return has_positive_pattern and has_excessive_punctuation

def detect_mocking_laughter_with_offensive_terms(text):
    """Detecta risadas de deboche com termos ofensivos - VERSÃƒO MELHORADA"""
    text_lower = text.lower()
    
    # Termos ofensivos
    offensive_terms = [
        r'\b(viado|bicha|sapatÃ£o|paneleiro|gay|lÃ©sbica|bissexual|queer)\b',
        r'\b(travesti|trans|transgÃªnero|transgenero)\b',
        r'\b(nojento|nojenta|escroto|escrota|desgraÃ§ado|desgraÃ§ada)\b',
        r'\b(doente|doenÃ§a|tratamento|cura|psicolÃ³gico|mental)\b',
        r'\b(pecado|deus|demÃ´nio|igreja|bÃ­blia|cristÃ£o)\b',
        r'\b(natural|normal|anormal|aberraÃ§Ã£o|erro)\b'
    ]
    
    # Emojis de risada (mÃºltiplos indicam deboche)
    laugh_emojis = ['ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜†', 'ğŸ˜„', 'ğŸ˜ƒ', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ¤ª']
    laugh_text = ['kkkk', 'haha', 'hehe', 'rsrs']
    
    # Contar emojis de risada
    laugh_emoji_count = sum(1 for emoji in laugh_emojis if emoji in text)
    laugh_text_count = sum(1 for laugh in laugh_text if laugh in text_lower)
    
    # Verificar se hÃ¡ termos ofensivos
    has_offensive_terms = any(re.search(pattern, text_lower) for pattern in offensive_terms)
    
    # SÃ³ Ã© hate se hÃ¡ termos ofensivos E risada
    if not has_offensive_terms:
        return False
    
    # Verificar se hÃ¡ risadas mÃºltiplas (deboche)
    has_mocking_laughter = laugh_emoji_count >= 2 or laugh_text_count >= 1
    
    return has_offensive_terms and has_mocking_laughter

def predict_hate_speech(text):
    """PrediÃ§Ã£o usando regras contextuais + modelo real treinado"""
    try:
        # 0. PRIMEIRO: Verificar casos que devem ser SEMPRE NÃƒO-HATE (ALTA PRIORIDADE)
        
        # Contexto positivo com emojis de apoio
        if detect_positive_context_with_emojis(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'positive_context_with_emojis_rule'
            }
        
        # PadrÃµes de orgulho LGBTQIA+
        if detect_orgulho_lgbtqia(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'orgulho_lgbtqia_rule'
            }
        
        # PadrÃµes de respeito e aceitaÃ§Ã£o
        if detect_respeito_aceitacao(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'respeito_aceitacao_rule'
            }
        
        # PalavrÃµes em contexto positivo
        if detect_curse_words_positive_context(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'curse_words_positive_context_rule'
            }
        
        # Linguagem neutra sozinha (NÃƒO Ã© hate)
        if detect_neutral_language_only(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'neutral_language_only_rule'
            }
        
        # Emoji sozinho ou com contexto mÃ­nimo
        if detect_single_emoji_context(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'single_emoji_context_rule'
            }
        
        # Respeito com boyceta
        if detect_respeito_boyceta(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'respeito_boyceta_rule'
            }
        
        # Apenas emojis positivos
        if detect_positive_emojis_only(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'positive_emojis_only_rule'
            }
        
        # Contexto positivo com pontuaÃ§Ã£o excessiva
        if detect_positive_context_with_punctuation(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'positive_context_with_punctuation_rule'
            }
        
        # 1. SEGUNDO: Verificar casos que devem ser SEMPRE HATE (ALTA PRIORIDADE)
        
        # Risadas de deboche com termos ofensivos
        if detect_mocking_laughter_with_offensive_terms(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.95,
                'method': 'mocking_laughter_with_offensive_terms_rule'
            }
        
        # Emojis de hate com risadas
        if detect_hate_emojis_with_laughter(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.95,
                'method': 'hate_emojis_with_laughter_rule'
            }
        
        # PalhaÃ§ada com risadas (hate contra linguagem neutra)
        if detect_palhacada_with_laughter(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "Transfobia",
                'confidence': 0.95,
                'method': 'palhacada_with_laughter_rule'
            }
        
        # PalavrÃµes cifrados
        if detect_ciphered_curse_words(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.95,
                'method': 'ciphered_curse_words_rule'
            }
        
        # Termos patologizantes com risadas
        if detect_pathologizing_with_laughter(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "Transfobia",
                'confidence': 0.95,
                'method': 'pathologizing_with_laughter_rule'
            }
        
        # 2. TERCEIRO: Verificar machismo atravÃ©s de genitais masculinos (ALTA PRIORIDADE)
        
        if detect_enhanced_male_genital_machismo(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.95,
                'method': 'enhanced_male_genital_machismo_rule'
            }
        
        # 1. SEGUNDO: Verificar Ã³dio contra linguagem neutra (ALTA PRIORIDADE)
        
        if detect_enhanced_neutral_language_hate(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "Transfobia",
                'confidence': 0.95,
                'method': 'enhanced_neutral_language_hate_rule'
            }
        
        # 2. TERCEIRO: Verificar casos que devem ser NÃƒO-HATE (alta prioridade para reduzir falsos positivos)
        
        if detect_care_expressions(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'care_expressions_rule'
            }
        
        if detect_neutral_curse_words(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'neutral_curse_words_rule'
            }
        
        if detect_disapproval_without_hate(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'disapproval_without_hate_rule'
            }
        
        if detect_generic_insults_without_context(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'generic_insults_without_context_rule'
            }
        
        if detect_neutral_emoji_context(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'neutral_emoji_context_rule'
            }
        
        if detect_neutral_language_specific_cases(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'neutral_language_specific_cases_rule'
            }
        
        # 1. SEGUNDO: Verificar casos especÃ­ficos problemÃ¡ticos identificados pelo usuÃ¡rio
        
        # Casos que devem ser HATE
        if detect_generation_expressions(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.95,
                'method': 'generation_expressions_rule'
            }
        
        if detect_male_genital_machismo(text):
            return {
                'is_hate': True,
                'hate_probability': 0.90,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.90,
                'method': 'male_genital_machismo_rule'
            }
        
        # Emoji de palhaÃ§o isolado pode ser neutro ou hate dependendo do contexto
        # Se for apenas o emoji, considerar como nÃ£o-hate
        if text.strip() == 'ğŸ¤¡':
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'clown_emoji_isolated_neutral_rule'
            }
        
        if detect_neutral_language_opposition(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "Transfobia",
                'confidence': 0.95,
                'method': 'neutral_language_opposition_rule'
            }
        
        if detect_clown_emoji_context(text):
            return {
                'is_hate': True,
                'hate_probability': 0.90,
                'specialized_class': "Transfobia", 
                'confidence': 0.90,
                'method': 'clown_emoji_context_rule'
            }
        
        if detect_vomit_emoji_context(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.95,
                'method': 'vomit_emoji_context_rule'
            }
        
        # Casos que devem ser NÃƒO-HATE
        if detect_laughter_context_neutral(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'laughter_context_neutral_rule'
            }
        
        if detect_curse_words_neutral_context(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'curse_words_neutral_context_rule'
            }
        
        if detect_tiredness_expressions(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'tiredness_expressions_rule'
            }
        
        if detect_religious_neutral_expressions(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'religious_neutral_expressions_rule'
            }
        
        # 1. SEGUNDO: Verificar emojis de hate (sempre hate)
        # Esta tem prioridade mÃ¡xima para detectar Ã³dio explÃ­cito
        if detect_hate_emojis(text):
            return {
                'is_hate': True,
                'hate_probability': 0.95,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.95,
                'method': 'hate_emoji_rule'
            }
        
        # 2. SEGUNDO: Verificar emojis de apoio (nÃ£o Ã© hate)
        # Esta tem prioridade alta para proteger apoio legÃ­timo
        if detect_supportive_emojis(text):
            return {
                'is_hate': False,
                'hate_probability': 0.01,
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'supportive_emoji_rule'
            }
        
        # 3. TERCEIRO: Verificar emojis de deboche (possÃ­vel hate)
        if detect_mocking_emojis(text) or detect_text_mocking_patterns(text):
            return {
                'is_hate': True,
                'hate_probability': 0.90,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.90,
                'method': 'mocking_emoji_rule'
            }
        
        # 4. QUARTO: Verificar comandos condescendentes (geralmente hate)
        if detect_condescending_commands(text):
            return {
                'is_hate': True,
                'hate_probability': 0.85,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.85,
                'method': 'condescending_command_rule'
            }
        
        # 5. QUINTO: Verificar moralismo religioso (geralmente hate)
        if detect_religious_moralism(text):
            return {
                'is_hate': True,
                'hate_probability': 0.80,
                'specialized_class': "Transfobia",
                'confidence': 0.80,
                'method': 'religious_moralism_rule'
            }
        
        # 6. SEXTO: Verificar termos patologizantes (geralmente hate)
        if detect_pathologizing_terms(text):
            return {
                'is_hate': True,
                'hate_probability': 0.85,
                'specialized_class': "Transfobia",
                'confidence': 0.85,
                'method': 'pathologizing_terms_rule'
            }
        
        # 7. SÃ‰TIMO: Verificar hate disfarÃ§ado (geralmente hate)
        if detect_disguised_hate(text):
            return {
                'is_hate': True,
                'hate_probability': 0.90,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.90,
                'method': 'disguised_hate_rule'
            }
        
        # 8. OITAVO: Verificar termos de vergonha (geralmente hate)
        if detect_shame_terms(text):
            return {
                'is_hate': True,
                'hate_probability': 0.80,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.80,
                'method': 'shame_terms_rule'
            }
        
        # 9. NONO: Verificar palavrÃµes (geralmente hate)
        if detect_curse_words(text):
            return {
                'is_hate': True,
                'hate_probability': 0.90,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.90,
                'method': 'curse_words_rule'
            }
        
        # 10. DÃ‰CIMO: Verificar termos machistas (geralmente hate)
        if detect_misogynistic_terms(text):
            return {
                'is_hate': True,
                'hate_probability': 0.85,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.85,
                'method': 'misogynistic_terms_rule'
            }
        
        # 11. DÃ‰CIMO PRIMEIRO: Verificar metÃ¡foras condescendentes (geralmente hate)
        if detect_condescending_metaphors(text):
            return {
                'is_hate': True,
                'hate_probability': 0.80,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.80,
                'method': 'condescending_metaphors_rule'
            }
        
        # 12. DÃ‰CIMO SEGUNDO: Verificar insultos condescendentes (geralmente hate)
        if detect_condescending_insults(text):
            return {
                'is_hate': True,
                'hate_probability': 0.85,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.85,
                'method': 'condescending_insults_rule'
            }
        
        # 13. DÃ‰CIMO TERCEIRO: Verificar excessos de pontuaÃ§Ã£o (geralmente hate)
        if detect_excessive_punctuation(text):
            return {
                'is_hate': True,
                'hate_probability': 0.75,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.75,
                'method': 'excessive_punctuation_rule'
            }
        
        # 14. DÃ‰CIMO QUARTO: Verificar insultos diretos (geralmente hate)
        if detect_direct_insults(text):
            return {
                'is_hate': True,
                'hate_probability': 0.90,
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.90,
                'method': 'direct_insults_rule'
            }
        
        # 4. QUARTO: Aplicar regras contextuais para termos de gÃªnero
        contextual_result = enhanced_hybrid_rules(text)
        
        if contextual_result == "nÃ£o_hate":
            return {
                'is_hate': False,
                'hate_probability': 0.01,  # Muito baixa para nÃ£o-hate
                'specialized_class': "N/A",
                'confidence': 0.99,
                'method': 'contextual_rule_positive'
            }
        elif contextual_result == "hate":
            return {
                'is_hate': True,
                'hate_probability': 0.95,  # Alta para hate contextual
                'specialized_class': "Transfobia",  # Assumir transfobia para termos de gÃªnero
                'confidence': 0.95,
                'method': 'contextual_rule_negative'
            }
        elif contextual_result == "assedio_insulto":
            return {
                'is_hate': True,
                'hate_probability': 0.95,  # Alta para hate contextual
                'specialized_class': "AssÃ©dio/Insulto",
                'confidence': 0.95,
                'method': 'contextual_rule_insult'
            }
        
        # 2. SEGUNDO: Se nÃ£o hÃ¡ regra contextual, usar modelo normal
        # Normalizar texto
        normalized_text = normalize_text(text)
        
        # Tokenizar
        inputs = tokenizer_binary(normalized_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        
        # PrediÃ§Ã£o binÃ¡ria
        with torch.no_grad():
            outputs_binary = model_binary(**inputs)
            binary_probs = torch.softmax(outputs_binary.logits, dim=-1)
            hate_probability = binary_probs[0][1].item()
        
        # Threshold otimizado baseado nos testes
        THRESHOLD = 0.05  # Reduzido de 0.15 para 0.05
        
        # Verificar se Ã© um falso positivo potencial
        if (hate_probability >= THRESHOLD and 
            is_lgbtqia_pattern(text) and 
            has_positive_adjective(text)):
            
            # Reduzir drasticamente a probabilidade para adjetivos positivos
            hate_probability = 0.01  # 1% - praticamente NÃƒO-HATE
        
        is_hate = hate_probability >= THRESHOLD
        
        # Se Ã© hate, fazer prediÃ§Ã£o especializada
        if is_hate:
            inputs_specialized = tokenizer_specialized(normalized_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            with torch.no_grad():
                outputs_specialized = model_specialized(**inputs_specialized)
                specialized_probs = torch.softmax(outputs_specialized.logits, dim=-1)
                specialized_pred = torch.argmax(specialized_probs, dim=-1)
            
            # Mapear classes especializadas
            class_mapping = {0: "Transfobia", 1: "AssÃ©dio/Insulto"}
            specialized_class = class_mapping.get(specialized_pred.item(), "AssÃ©dio/Insulto")
        else:
            specialized_class = "N/A"
        
        confidence = max(hate_probability, 1-hate_probability)
        
        return {
            'is_hate': is_hate,
            'hate_probability': hate_probability,
            'specialized_class': specialized_class,
            'confidence': confidence,
            'method': 'model_prediction'
        }
        
    except Exception as e:
        print(f"Erro na prediÃ§Ã£o: {e}")
        return simulate_hate_detection(text)

# --- FunÃ§Ãµes de AnÃ¡lise ---
def analyze_single_text(text):
    """Analisa um Ãºnico texto"""
    if not text or not text.strip():
        return "âŒ Por favor, insira um texto para anÃ¡lise."
    
    result = predict_hate_speech(text)
    
    # Emoji baseado no resultado
    if result['is_hate']:
        emoji = "ğŸ”´"
        status = "HATE SPEECH DETECTADO"
        color = "#ff4444"
    else:
        emoji = "ğŸŸ¢"
        status = "NÃƒO Ã‰ HATE SPEECH"
        color = "#44ff44"
    
    # InformaÃ§Ãµes detalhadas
    details = f"""
    <div style="background-color: {color}20; padding: 15px; border-radius: 10px; margin: 10px 0;">
        <h3 style="color: {color}; margin-top: 0;">{emoji} {status}</h3>
        <p><strong>Probabilidade de Hate:</strong> {result['hate_probability']:.1%}</p>
        <p><strong>Classe Especializada:</strong> {result['specialized_class']}</p>
        <p><strong>ConfianÃ§a:</strong> {result['confidence']:.1%}</p>
        <p><strong>MÃ©todo:</strong> {result.get('method', 'model_prediction')}</p>
    </div>
    """
    
    return details

def analyze_batch_text(texts):
    """Analisa mÃºltiplos textos"""
    if not texts or not texts.strip():
        return "âŒ Por favor, insira textos para anÃ¡lise."
    
    # Separar textos por linha
    text_list = [line.strip() for line in texts.split('\n') if line.strip()]
    
    if not text_list:
        return "âŒ Nenhum texto vÃ¡lido encontrado."
    
    results = []
    hate_count = 0
    
    for i, text in enumerate(text_list, 1):
        result = predict_hate_speech(text)
        
        if result['is_hate']:
            emoji = "ğŸ”´"
            status = "HATE"
            hate_count += 1
        else:
            emoji = "ğŸŸ¢"
            status = "NÃƒO-HATE"
        
        results.append(f"{emoji} <strong>{i}.</strong> {status} ({result['hate_probability']:.1%}) - {text}")
    
    summary = f"""
    <div style="background-color: #f0f0f0; padding: 15px; border-radius: 10px; margin: 10px 0;">
        <h3>ğŸ“Š Resumo da AnÃ¡lise</h3>
        <p><strong>Total de textos:</strong> {len(text_list)}</p>
        <p><strong>Hate speech detectado:</strong> {hate_count}</p>
        <p><strong>Taxa de detecÃ§Ã£o:</strong> {hate_count/len(text_list):.1%}</p>
    </div>
    """
    
    return summary + "<br>".join(results)

# --- Interface Gradio ---
with gr.Blocks(
    title="Radar Social LGBTQIA+",
    theme=gr.themes.Soft(),
    css="""
    .main-header {
        text-align: center;
        background: linear-gradient(90deg, #ff6b6b, #4ecdc4);
        color: white;
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 20px;
    }
    .analysis-box {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 10px;
        border-left: 5px solid #007bff;
        margin: 10px 0;
    }
    """
) as interface:
    
    gr.HTML("""
    <div class="main-header">
        <h1>ğŸ³ï¸â€ğŸŒˆ Radar Social LGBTQIA+</h1>
        <p>Sistema de InteligÃªncia Artificial (AI) com Processamento de Linguagem Natural (PLN) e Machine Learning (ML)</p>
        <p>DetecÃ§Ã£o avanÃ§ada de hate speech com regras contextuais e proteÃ§Ã£o de termos de dissidÃªncia de gÃªnero</p>
    </div>
    """)
    
    # InformaÃ§Ãµes principais antes das abas
    gr.Markdown("""
    ## ğŸ¯ Funcionalidades
    
    - **ğŸ¤– InteligÃªncia Artificial (AI)**: Sistema automatizado de detecÃ§Ã£o de hate speech
    - **ğŸ§  Processamento de Linguagem Natural (PLN)**: AnÃ¡lise contextual de texto em portuguÃªs brasileiro
    - **ğŸ“Š Machine Learning (ML)**: Modelos BERTimbau fine-tuned com ensemble learning
    - **ğŸ” Regras Contextuais**: ProteÃ§Ã£o inteligente de termos de dissidÃªncia de gÃªnero
    - **âš¡ AnÃ¡lise Especializada**: ClassificaÃ§Ã£o entre Transfobia e AssÃ©dio/Insulto
    
    ## ğŸ”§ Tecnologia
    
    - **ğŸ—ï¸ Arquitetura**: Sistema Ensemble (BinÃ¡rio + Especializado)
    - **ğŸ§  Modelo Base**: BERTimbau (BERT em portuguÃªs brasileiro)
    - **ğŸ“ˆ Threshold Adaptativo**: OtimizaÃ§Ã£o dinÃ¢mica baseada em contexto
    - **ğŸ”„ Pipeline NLP**: NormalizaÃ§Ã£o, tokenizaÃ§Ã£o e anÃ¡lise semÃ¢ntica
    - **ğŸ¯ Regras HÃ­bridas**: CombinaÃ§Ã£o de ML e regras especÃ­ficas
    
    ## ğŸ“Š MÃ©tricas
    
    - **ğŸ¯ Accuracy**: 74.6% (2.053 exemplos testados)
    - **âš¡ Precision**: 44.5% | **ğŸ“ˆ Recall**: 86.0% | **ğŸ¯ F1-Score**: 58.6%
    - **ğŸ”§ Regras Contextuais**: 100% accuracy nos casos problemÃ¡ticos
    - **ğŸ“Š Processamento**: 2.6% casos com regras contextuais, 97.4% com modelo ML
    """)
    
    with gr.Tabs():
        with gr.TabItem("ğŸ” AnÃ¡lise Individual"):
            gr.Markdown("### Analise um texto especÃ­fico")
            
            text_input = gr.Textbox(
                label="Digite o texto para anÃ¡lise",
                placeholder="Ex: 'Orgulho de ser boyceta' ou 'Viado do caralho'",
                lines=3
            )
            
            analyze_btn = gr.Button("ğŸ” Analisar", variant="primary")
            
            result_output = gr.HTML(label="Resultado da AnÃ¡lise")
            
            analyze_btn.click(
                fn=analyze_single_text,
                inputs=text_input,
                outputs=result_output
            )
        
        with gr.TabItem("ğŸ“Š AnÃ¡lise em Lote"):
            gr.Markdown("### Analise mÃºltiplos textos (um por linha)")
            
            batch_input = gr.Textbox(
                label="Digite os textos para anÃ¡lise (um por linha)",
                placeholder="Ex:\nOrgulho de ser boyceta\nViado do caralho\nSapatÃ£o Ã© forÃ§a",
                lines=10
            )
            
            batch_analyze_btn = gr.Button("ğŸ“Š Analisar Lote", variant="primary")
            
            batch_result_output = gr.HTML(label="Resultado da AnÃ¡lise em Lote")
            
            batch_analyze_btn.click(
                fn=analyze_batch_text,
                inputs=batch_input,
                outputs=batch_result_output
            )
        
        with gr.TabItem("â„¹ï¸ InformaÃ§Ãµes TÃ©cnicas"):
            gr.Markdown("""
            ## ğŸ¯ Regras Contextuais
            
            ### ğŸ›¡ï¸ ProteÃ§Ã£o de Termos de GÃªnero
            - **"boyceta"**: Detecta contexto positivo vs negativo
            - **"sapatÃ£o"**: Protege identidade lÃ©sbica
            - **"travesti"**: Respeita identidade trans
            - **"transgÃªnero"**: AnÃ¡lise contextual de identidade
            
            ### ğŸ” Contextos Detectados
            - **âœ… Positivo**: orgulho, beleza, identidade, expressÃ£o
            - **âŒ Negativo**: nojo, escroto, desgraÃ§ado, arrombado
            - **ğŸ“š Educativo**: definiÃ§Ã£o, conceito, explicaÃ§Ã£o
            - **ğŸ˜„ RidicularizaÃ§Ã£o**: engraÃ§ado, hilÃ¡rio, cÃ´mico
            
            ### ğŸ§  Arquitetura do Sistema
            
            #### ğŸ¤– Modelos de Machine Learning
            - **Modelo BinÃ¡rio**: Detecta hate vs nÃ£o-hate (BERTimbau)
            - **Modelo Especializado**: Classifica tipo de hate (2 classes)
            - **Ensemble Learning**: CombinaÃ§Ã£o de mÃºltiplos modelos
            - **Fine-tuning**: AdaptaÃ§Ã£o especÃ­fica para portuguÃªs brasileiro
            
            #### ğŸ”„ Pipeline de Processamento
            1. **NormalizaÃ§Ã£o**: URLs, menÃ§Ãµes e hashtags substituÃ­das
            2. **TokenizaÃ§Ã£o**: ConversÃ£o para tokens numÃ©ricos
            3. **AnÃ¡lise Contextual**: AplicaÃ§Ã£o de regras especÃ­ficas
            4. **ClassificaÃ§Ã£o ML**: PrediÃ§Ã£o com modelos treinados
            5. **PÃ³s-processamento**: Ajustes baseados em contexto
            
            ### ğŸ“Š Base de Dados
            
            #### ğŸ—ƒï¸ Fontes Integradas
            - **AnotaÃ§Ãµes Manuais**: Equipe CÃ³digo NÃ£o BinÃ¡rio
            - **Dataset ToLD-BR**: Dados acadÃªmicos em portuguÃªs
            - **Dataset Anti-LGBT**: Cyberbullying traduzido para PT-BR
            - **Dados Reais**: Instagram do podcast Entre Amigues
            
            #### ğŸ“ˆ EstatÃ­sticas
            - **Total de exemplos**: ~15.000 comentÃ¡rios
            - **Dataset expandido**: 4.780.095 exemplos
            - **ValidaÃ§Ã£o**: Testado com dados reais de hate speech
            - **AnonimizaÃ§Ã£o**: Dados pessoais removidos para privacidade
            
            ### âš ï¸ ConsideraÃ§Ãµes Ã‰ticas
            
            #### ğŸ”’ Privacidade e Conformidade
            - **LGPD/GDPR**: CompatÃ­vel com regulamentaÃ§Ãµes de privacidade
            - **AnonimizaÃ§Ã£o**: IDs substituÃ­dos por hashes
            - **NormalizaÃ§Ã£o**: MenÃ§Ãµes (@usuario) e URLs removidas
            - **TransparÃªncia**: Metodologia aberta e auditÃ¡vel
            
            #### ğŸ¯ Uso ResponsÃ¡vel
            - **Foco Social**: Combate ao discurso de Ã³dio LGBTQIA+
            - **ProteÃ§Ã£o**: Termos de identidade de gÃªnero respeitados
            - **EducaÃ§Ã£o**: Ferramenta de apoio para moderaÃ§Ã£o
            - **Impacto**: Baseado em dados reais de Ã³dio sofrido
            
            ### ğŸ”— Links e Recursos
            
            #### ğŸ“š Projetos Relacionados
            - [Modelo no Hugging Face](https://hf.co/Veronyka/radar-social-lgbtqia)
            - [Dataset no Hugging Face](https://hf.co/datasets/Veronyka/base-dados-odio-lgbtqia)
            - [RepositÃ³rio GitHub (Modelo)](https://github.com/travahacker/radar-social-lgbtqia)
            - [RepositÃ³rio GitHub (Dataset)](https://github.com/travahacker/base-dados-odio-lgbtqia)
            
            #### ğŸ³ï¸â€ğŸŒˆ CÃ³digo NÃ£o BinÃ¡rio
            - [Site Oficial](https://codigonaobinario.org)
            - [Podcast Entre Amigues](https://linktr.ee/entre_amigues)
            
            ---
            
            **Desenvolvido com â¤ï¸ pela equipe CÃ³digo NÃ£o BinÃ¡rio**
            """)
    

if __name__ == "__main__":
    interface.launch()
