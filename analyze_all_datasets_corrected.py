#!/usr/bin/env python3
"""
Script para an√°lise completa com corre√ß√µes aplicadas nos tr√™s datasets
"""

import pandas as pd
import os
from datetime import datetime
from app_space_version import predict_hate_speech

def analyze_all_datasets():
    """Analisa todos os tr√™s datasets com as corre√ß√µes aplicadas"""
    
    # Arquivos para an√°lise
    datasets = {
        'Instagram': 'clean-annotated-data/export_1757023553205_limpa.csv',
        'TikTok': 'clean-annotated-data/tiktok_consolidado_limpo_20251016_181651.csv',
        'YouTube': 'clean-annotated-data/youtube_limpo_20251016_181656.csv'
    }
    
    # Colunas de texto para cada dataset
    text_columns = {
        'Instagram': 'Comment Text',
        'TikTok': 'text',
        'YouTube': 'text'
    }
    
    print("üöÄ AN√ÅLISE COMPLETA COM CORRE√á√ïES APLICADAS")
    print("=" * 60)
    
    all_results = []
    
    for platform, file_path in datasets.items():
        print(f"\nüì± Analisando {platform.upper()}...")
        print(f"üìÇ Arquivo: {file_path}")
        
        try:
            # Carregar dados com separador correto
            if platform == 'Instagram':
                df = pd.read_csv(file_path, sep=';')
            else:
                df = pd.read_csv(file_path)
            print(f"üìä Total de coment√°rios: {len(df):,}")
            
            # Verificar coluna de texto
            text_col = text_columns[platform]
            if text_col not in df.columns:
                print(f"‚ùå Coluna '{text_col}' n√£o encontrada!")
                continue
            
            # Lista para armazenar resultados
            results = []
            
            print("üîç Iniciando an√°lise com sistema corrigido...")
            
            # Processar cada coment√°rio
            for idx, row in df.iterrows():
                if idx % 100 == 0:
                    print(f"üìà Processando coment√°rio {idx+1:,}/{len(df):,}")
                
                text = str(row[text_col])
                
                try:
                    # Usar sistema corrigido
                    result = predict_hate_speech(text)
                    
                    # Adicionar resultado
                    result_row = {
                        'platform': platform,
                        'id': row.get('id', idx + 1),
                        'text': text,
                        'text_length': len(text),
                        'text_features': f"Length: {len(text)}, Words: {len(text.split())}, Has_emoji: {'emoji' in text.lower()}",
                        'predicted_label': 'HATE' if result['is_hate'] else 'N√ÉO-HATE',
                        'method': result['method'],
                        'specialized_class': result['specialized_class'],
                        'confidence': result['confidence'],
                        'hate_probability': result['hate_probability']
                    }
                    
                    # Adicionar colunas espec√≠ficas de cada plataforma
                    if platform == 'Instagram':
                        result_row.update({
                            'author_handle': row.get('Author Handle', ''),
                            'like_count': row.get('Like Count', ''),
                            'timestamp': row.get('Timestamp', '')
                        })
                    elif platform == 'TikTok':
                        result_row.update({
                            'author_handle': row.get('author_handle', ''),
                            'like_count_visible': row.get('like_count_visible', ''),
                            'timestamp_visible': row.get('timestamp_visible', ''),
                            'video_id': row.get('video_id', '')
                        })
                    elif platform == 'YouTube':
                        result_row.update({
                            'titulo_video': row.get('titulo_video', ''),
                            'data': row.get('data', ''),
                            'likes_comentario': row.get('likes_comentario', ''),
                            'autor_handle': row.get('autor_handle', '')
                        })
                    
                    results.append(result_row)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è  Erro ao processar coment√°rio {idx+1}: {str(e)}")
                    # Adicionar resultado de erro
                    result_row = {
                        'platform': platform,
                        'id': row.get('id', idx + 1),
                        'text': text,
                        'text_length': len(text),
                        'text_features': f"Length: {len(text)}, Words: {len(text.split())}, Has_emoji: {'emoji' in text.lower()}",
                        'predicted_label': 'ERRO',
                        'method': 'error',
                        'specialized_class': 'N/A',
                        'confidence': 0.0,
                        'hate_probability': 0.0
                    }
                    results.append(result_row)
            
            # Criar DataFrame com resultados
            results_df = pd.DataFrame(results)
            
            # Salvar resultados individuais
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"out/ANALISE_{platform.upper()}_CORRIGIDO_{timestamp}.csv"
            results_df.to_csv(output_file, index=False, encoding='utf-8')
            
            print(f"üíæ Resultados salvos: {output_file}")
            
            # Estat√≠sticas
            total_comments = len(results_df)
            hate_comments = len(results_df[results_df['predicted_label'] == 'HATE'])
            nao_hate_comments = len(results_df[results_df['predicted_label'] == 'N√ÉO-HATE'])
            error_comments = len(results_df[results_df['predicted_label'] == 'ERRO'])
            
            print(f"\nüìà Estat√≠sticas da an√°lise ({platform}):")
            print(f"   - Total de coment√°rios: {total_comments:,}")
            print(f"   - Coment√°rios HATE: {hate_comments:,} ({hate_comments/total_comments*100:.1f}%)")
            print(f"   - Coment√°rios N√ÉO-HATE: {nao_hate_comments:,} ({nao_hate_comments/total_comments*100:.1f}%)")
            print(f"   - Coment√°rios com ERRO: {error_comments:,} ({error_comments/total_comments*100:.1f}%)")
            
            # Distribui√ß√£o por m√©todo
            print(f"\nüîß Top m√©todos de detec√ß√£o ({platform}):")
            method_counts = results_df['method'].value_counts()
            for method, count in method_counts.head(5).items():
                print(f"   - {method}: {count:,} ({count/total_comments*100:.1f}%)")
            
            # Distribui√ß√£o por classe especializada (apenas HATE)
            hate_df = results_df[results_df['predicted_label'] == 'HATE']
            if len(hate_df) > 0:
                print(f"\nüéØ Distribui√ß√£o por classe especializada ({platform}):")
                class_counts = hate_df['specialized_class'].value_counts()
                for class_name, count in class_counts.items():
                    if class_name != 'N/A':
                        print(f"   - {class_name}: {count:,} ({count/len(hate_df)*100:.1f}%)")
            
            # Adicionar √† lista geral
            all_results.extend(results)
            
        except Exception as e:
            print(f"‚ùå Erro ao analisar {platform}: {str(e)}")
            continue
    
    # Criar relat√≥rio consolidado
    if all_results:
        print(f"\nüìä GERANDO RELAT√ìRIO CONSOLIDADO...")
        
        all_results_df = pd.DataFrame(all_results)
        
        # Salvar relat√≥rio consolidado
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        consolidated_file = f"out/ANALISE_CONSOLIDADA_CORRIGIDA_{timestamp}.csv"
        all_results_df.to_csv(consolidated_file, index=False, encoding='utf-8')
        
        print(f"üíæ Relat√≥rio consolidado salvo: {consolidated_file}")
        
        # Estat√≠sticas gerais
        total_all = len(all_results_df)
        hate_all = len(all_results_df[all_results_df['predicted_label'] == 'HATE'])
        nao_hate_all = len(all_results_df[all_results_df['predicted_label'] == 'N√ÉO-HATE'])
        
        print(f"\nüåê ESTAT√çSTICAS GERAIS (TODAS AS PLATAFORMAS):")
        print(f"   - Total de coment√°rios: {total_all:,}")
        print(f"   - Coment√°rios HATE: {hate_all:,} ({hate_all/total_all*100:.1f}%)")
        print(f"   - Coment√°rios N√ÉO-HATE: {nao_hate_all:,} ({nao_hate_all/total_all*100:.1f}%)")
        
        # Estat√≠sticas por plataforma
        print(f"\nüì± ESTAT√çSTICAS POR PLATAFORMA:")
        for platform in ['Instagram', 'TikTok', 'YouTube']:
            platform_df = all_results_df[all_results_df['platform'] == platform]
            if len(platform_df) > 0:
                platform_hate = len(platform_df[platform_df['predicted_label'] == 'HATE'])
                platform_total = len(platform_df)
                print(f"   - {platform}: {platform_hate:,}/{platform_total:,} ({platform_hate/platform_total*100:.1f}% HATE)")
        
        return consolidated_file
    
    return None

if __name__ == "__main__":
    print("üöÄ Iniciando an√°lise completa com corre√ß√µes...")
    output_file = analyze_all_datasets()
    if output_file:
        print(f"‚úÖ An√°lise completa conclu√≠da! Arquivo: {output_file}")
    else:
        print("‚ùå Falha na an√°lise completa")
